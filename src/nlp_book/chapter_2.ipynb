{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Tokenization using NLTK\n",
    "\n",
    "This notebook demonstrates various text tokenization techniques using NLTK and spaCy libraries. We'll explore different approaches to breaking down text into meaningful tokens and analyze their features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the required libraries and download necessary NLTK data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\giloz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk import word_tokenize\n",
    "from spacy import displacy\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Texts\n",
    "\n",
    "We'll use a literary excerpt and several test sentences with various linguistic features to demonstrate different tokenization approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original example\n",
    "text = (\n",
    "    \"Trust me, though, the words were on their way, and when \"\n",
    "    \"they arrived, Liesel would hold them in her hands like \"\n",
    "    \"the clouds, and she would wring them out, like the rain.\"\n",
    ")\n",
    "\n",
    "# New interesting sentences with various linguistic features\n",
    "new_sentences = [\n",
    "    \"The AI researcher's model achieved 99.9% accuracy - a groundbreaking result!\",\n",
    "    \"Mr. Smith bought a Ph.D. degree from example.com for $9,999...\",\n",
    "    \"She exclaimed, 'OMG! This can't be real!' while reading the email.\",\n",
    "    \"The code runs fast (about 2.5x faster) than our previous implementation.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic String Tokenization\n",
    "\n",
    "Let's start with Python's built-in string splitting method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic split() tokenization:\n",
      "['Trust', 'me,', 'though,', 'the', 'words', 'were', 'on', 'their']\n"
     ]
    }
   ],
   "source": [
    "print(\"Basic split() tokenization:\")\n",
    "tokens = text.split()\n",
    "print(tokens[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expression Tokenization\n",
    "\n",
    "Using regex for more sophisticated tokenization that can handle contractions and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regex tokenization:\n",
      "['The', 'code', 'runs', 'fast', '(', 'about', '2', '.', '5x', 'faster', ')', 'than', 'our', 'previous', 'implementation', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Regex tokenization:\")\n",
    "pattern = r\"\\w+(?:'\\w+)?|[^\\w\\s]\"\n",
    "texts = [text] + new_sentences\n",
    "tokens = list(re.findall(pattern, texts[-1]))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Tokenization\n",
    "\n",
    "NLTK provides more sophisticated tokenization that handles various edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK tokenization:\n",
      "\n",
      "Original: The AI researcher's model achieved 99.9% accuracy - a groundbreaking result!\n",
      "Tokens: ['The', 'AI', 'researcher', \"'s\", 'model', 'achieved', '99.9', '%', 'accuracy', '-', 'a', 'groundbreaking', 'result', '!']\n",
      "\n",
      "Original: Mr. Smith bought a Ph.D. degree from example.com for $9,999...\n",
      "Tokens: ['Mr.', 'Smith', 'bought', 'a', 'Ph.D.', 'degree', 'from', 'example.com', 'for', '$', '9,999', '...']\n",
      "\n",
      "Original: She exclaimed, 'OMG! This can't be real!' while reading the email.\n",
      "Tokens: ['She', 'exclaimed', ',', \"'OMG\", '!', 'This', 'ca', \"n't\", 'be', 'real', '!', \"'\", 'while', 'reading', 'the', 'email', '.']\n",
      "\n",
      "Original: The code runs fast (about 2.5x faster) than our previous implementation.\n",
      "Tokens: ['The', 'code', 'runs', 'fast', '(', 'about', '2.5x', 'faster', ')', 'than', 'our', 'previous', 'implementation', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"NLTK tokenization:\")\n",
    "for sentence in new_sentences:\n",
    "    print(\"\\nOriginal:\", sentence)\n",
    "    print(\"Tokens:\", word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy Tokenization and Analysis\n",
    "\n",
    "SpaCy provides comprehensive NLP capabilities including tokenization and linguistic feature analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy tokenization:\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "['The', 'code', 'runs', 'fast', '(', 'about', '2.5x', 'faster', ')', 'than', 'our', 'previous', 'implementation', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"SpaCy tokenization:\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(texts[-1])\n",
    "print(type(doc))\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced SpaCy Features\n",
    "\n",
    "Let's explore SpaCy's additional linguistic analysis capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy's advanced features:\n",
      "Token: The             | Lemma: the             | POS: DET        | Tag: DT        \n",
      "Token: code            | Lemma: code            | POS: NOUN       | Tag: NN        \n",
      "Token: runs            | Lemma: run             | POS: VERB       | Tag: VBZ       \n",
      "Token: fast            | Lemma: fast            | POS: ADV        | Tag: RB        \n",
      "Token: (               | Lemma: (               | POS: PUNCT      | Tag: -LRB-     \n",
      "Token: about           | Lemma: about           | POS: ADV        | Tag: RB        \n",
      "Token: 2.5x            | Lemma: 2.5x            | POS: NOUN       | Tag: NN        \n",
      "Token: faster          | Lemma: fast            | POS: ADV        | Tag: RBR       \n",
      "Token: )               | Lemma: )               | POS: PUNCT      | Tag: -RRB-     \n",
      "Token: than            | Lemma: than            | POS: ADP        | Tag: IN        \n",
      "Token: our             | Lemma: our             | POS: PRON       | Tag: PRP$      \n",
      "Token: previous        | Lemma: previous        | POS: ADJ        | Tag: JJ        \n",
      "Token: implementation  | Lemma: implementation  | POS: NOUN       | Tag: NN        \n",
      "Token: .               | Lemma: .               | POS: PUNCT      | Tag: .         \n"
     ]
    }
   ],
   "source": [
    "print(\"SpaCy's advanced features:\")\n",
    "for token in doc:\n",
    "    print(f\"Token: {token.text:15} | Lemma: {token.lemma_:15} | POS: {token.pos_:10} | Tag: {token.tag_:10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Parsing Visualization\n",
    "\n",
    "Finally, let's visualize the sentence structure using SpaCy's displacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'display' from 'IPython.core.display' (c:\\dev\\ai_experiments\\code_cademy_nlp\\venv\\Lib\\site-packages\\IPython\\core\\display.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m sentence_span = \u001b[38;5;28mlist\u001b[39m(doc.sents)[\u001b[32m0\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m svg = \u001b[43mdisplacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_span\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdep\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjupyter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Path(\u001b[33m\"\u001b[39m\u001b[33msentence_diagram.svg\u001b[39m\u001b[33m\"\u001b[39m).open(\u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      4\u001b[39m     f.write(svg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\dev\\ai_experiments\\code_cademy_nlp\\venv\\Lib\\site-packages\\spacy\\displacy\\__init__.py:69\u001b[39m, in \u001b[36mrender\u001b[39m\u001b[34m(docs, style, page, minify, jupyter, options, manual)\u001b[39m\n\u001b[32m     65\u001b[39m     html = RENDER_WRAPPER(html)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m jupyter \u001b[38;5;129;01mor\u001b[39;00m (jupyter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_in_jupyter()):\n\u001b[32m     67\u001b[39m     \u001b[38;5;66;03m# return HTML rendered by IPython display()\u001b[39;00m\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# See #4840 for details on span wrapper to disable mathjax\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HTML, display\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m display(HTML(\u001b[33m'\u001b[39m\u001b[33m<span class=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtex2jax_ignore\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m>\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m</span>\u001b[39m\u001b[33m'\u001b[39m.format(html)))\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m html\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'display' from 'IPython.core.display' (c:\\dev\\ai_experiments\\code_cademy_nlp\\venv\\Lib\\site-packages\\IPython\\core\\display.py)"
     ]
    }
   ],
   "source": [
    "sentence_span = list(doc.sents)[0]\n",
    "svg = displacy.render(sentence_span, style=\"dep\", jupyter=True)\n",
    "with Path(\"sentence_diagram.svg\").open(\"w\") as f:\n",
    "    f.write(svg)\n",
    "displacy.render(sentence_span, style=\"dep\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
